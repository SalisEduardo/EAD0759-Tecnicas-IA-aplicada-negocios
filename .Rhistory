p <- ggplot(carseats, aes(y=Sales, x=!!sym(col))) +
geom_point() +
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "slateblue")
print(p1)
}
carseats |>
ggplot( aes(x=Age, y=Sales)) +
geom_point() +
geom_smooth(method=lm , color="red", se=FALSE) +
theme_classic()
carseats_numeric <-subset(carseats, select = -c(ShelveLoc,US,Urban,High))
ggpairs(carseats_numeric, title="Correlograma")
ggcorr(carseats_numeric, method = c("everything", "pearson"),label = TRUE)
p <- ggplot(carseats, aes(y=Price, x=CompPrice)) +
geom_point() +
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "slateblue")
print(p1)
ggpairs(subset(carseats, select = -c(ShelveLoc,US,Urban,Sales)), ggplot2::aes(colour=High,alpha=0.5),upper = list(continuous = wrap("cor", size = 3)))
ggpairs(subset(carseats, select = -c(ShelveLoc,US,Urban,Sales)), ggplot2::aes(colour=High,alpha=0.5),upper = list(continuous = wrap("cor", size = 3)), lower = "blank")
carseats_trainIndex <- createDataPartition(carseats$High, p = 0.75, list = FALSE)
training_carseats <- carseats[carseats_trainIndex, ] |>
dplyr::select(-Sales)
test_carseats <- carseats[-carseats_trainIndex, ]|>
dplyr::select(-Sales)
print("Train dimentions: ")
dim(training_carseats)
print("Test dimentions: ")
dim(test_carseats)
head(training_carseats)
head(test_carseats)
set.seed(42)
options(repr.plot.width=10, repr.plot.height=8)  # set plot size
tree_carseats_train = tree(High ~ ., data= training_carseats)
plot(tree_carseats_train )
text(tree_carseats_train , pretty=0,cex=0.55)
predict_train =  predict(tree_carseats_train, training_carseats, type = "class")
conf_matrix_train <- confusionMatrix(data = predict_train, reference = training_carseats$High)
conf_matrix_train
predict_test <- predict(tree_carseats_train,test_carseats,type='class') #prevendo coms os o modelo do treino
conf_matrix_test <- caret::confusionMatrix(data = predict_test,reference = test_carseats$High)
conf_matrix_test
# checando o balanceamento dos dados
test_carseats |>
count(High) |>
mutate(prop= n/ sum(n))
cv.carseats = cv.tree(tree_carseats_train, FUN = prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats = prune.misclass(tree_carseats_train, best = 12)
plot(prune.carseats)
text(prune.carseats, pretty=0)
prune_tree_pred = predict(prune.carseats, test_carseats, type="class")
prune_conf_matrix <- caret::confusionMatrix(data = prune_tree_pred,reference = test_carseats$High)
prune_conf_matrix
install.packages("randomForest")
?require
boston <- MASS::Boston
boston |> head()
boston |> str() # tipos dos dados
boston |> dim() # tamanho da tabela
boston |> is.na()  |> sum()
boston |> summary()
boston |> str()
par(mfrow=c(4,4))
for (i in 1:14) {
hist(boston[,i], main=colnames(Boston)[i])
}
par(mfrow=c(4,4))
for (i in 1:14) {
hist(boston[,i], main=colnames(Boston)[i])
}
par(mfrow=c(4,4))
for (i in 1:14) {
hist(boston[,i], main=colnames(boston)[i])
}
par(mfrow=c(4,4), mar=c(2,2,2,2))
for (i in 1:14) {
hist(boston[,i], main=colnames(boston)[i])
}
for(col in names(boston)){
p <- ggplot(carseats, aes(y=medv, x=!!sym(col))) +
geom_point() +
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "slateblue")
print(p1)
}
for(col in names(boston)){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "slateblue")
print(p1)
}
boston[,-medv]
boston[,-"medv"]
boston[-"medv"]
boston[,-c("medv")]
boston$medv
select(boston,-c(medv))
select(boston,-c(medv)) |> names
select(boston,-c(medv)) |> names()
for(col in names(boston)){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen") + labs(title = paste("medv vs ",col))
print(p1)
}
for(col in names(boston)){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
for(col in names(select(boston,-c(medv)))){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
for(col in names(select(boston,-c(medv)))){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
geom_smooth(method=lm , color="red", se=FALSE) +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
for(col in names(select(boston,-c(medv)))){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
#geom_smooth(method=lm , color="red", se=FALSE) +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
?Boston
p <- ggplot(boston, aes(y=black, x=crim)) +
geom_point() +
geom_smooth(method=lm , color="red", se=FALSE) +
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
p1
for(col in names(select(boston,-c(medv)))){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
#geom_smooth(method=lm , color="red", se=FALSE) +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
ggplot(boston, aes(x = variable, y = value)) +
geom_boxplot() +
facet_wrap(~ variable, scales = "free") +
labs(x = "Variable", y = "Value", title = "Box Plots for All Variables")
ggplot(tidyr::gather(boston), aes(x = variable, y = value)) +
geom_boxplot() +
facet_wrap(~ variable, scales = "free") +
labs(x = "Variable", y = "Value", title = "Box Plots for All Variables")
tidyr::gather(boston)
ggplot(tidyr::gather(boston), aes(x = key, y = value)) +
geom_boxplot() +
facet_wrap(~ key, scales = "free") +
labs(x = "Variable", y = "Value", title = "Box Plots for All Variables")
ggcorr(boston, method = c("everything", "pearson"),label = TRUE)
boston_trainIndex <- createDataPartition(boston$medv, p = 0.75, list = FALSE)
training_boston <- boston[boston_trainIndex, ] |>
dplyr::select(-Sales)
boston_trainIndex <- createDataPartition(boston$medv, p = 0.75, list = FALSE)
training_boston <- boston[boston_trainIndex, ]
test_boston <- boston[-boston_trainIndex, ]
print("Train dimentions: ")
dim(training_boston)
print("Test dimentions: ")
dim(test_boston)
head(training_boston)
head(test_boston)
boston_trainIndex <- createDataPartition(boston$medv, p = 0.85, list = FALSE)
training_boston <- boston[boston_trainIndex, ]
test_boston <- boston[-boston_trainIndex, ]
print("Train dimentions: ")
dim(training_boston)
print("Test dimentions: ")
dim(test_boston)
head(training_boston)
head(test_boston)
boston_trainIndex <- createDataPartition(boston$medv, p = 0.80, list = FALSE)
training_boston <- boston[boston_trainIndex, ]
test_boston <- boston[-boston_trainIndex, ]
print("Train dimentions: ")
dim(training_boston)
print("Test dimentions: ")
dim(test_boston)
head(training_boston)
head(test_boston)
rf.boston = randomForest(medv~., data = training_boston)
library(MASS)
require(randomForest)
set.seed(42)
boston <- MASS::Boston
library(MASS)
library(randomForest)
set.seed(42)
boston <- MASS::Boston
rf.boston = randomForest::randomForest(medv~., data = training_boston)
rf.boston
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry,
ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
dim(training_boston)
names(training_boston)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = training_boston, mtry=mtry,
ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, test_boston)
test.err[mtry] = with(test_boston, mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red",
"blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red",
"blue"))
min(oob.err)
erros_rf  <- data.frame(
mtry = mtry,
mse_oob = oob.err,
mse_test = test.err
)
erros_rf
erros_rf  <- data.frame(
mtry = 1:mtry,
mse_oob = oob.err,
mse_test = test.err
)
erros_rf
erros_rf |>
dplyr::filter(mse_oob == min(mse_oob)) |>
dplyr::pull(mtry)
erros_rf
cbind(test.err, oob.err)
test.err
double(13)
set.seed(101)
train = sample(1:nrow(boston), 300)
rf.boston = randomForest(medv~., data = boston, subset = train)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry,
ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red",
"blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red",
"blue"))
oob.err
test.err
set.seed(42)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = training_boston, mtry=mtry,
ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, test_boston)
test.err[mtry] = with(test_boston, mean( (medv-pred)^2 ))
}
set.seed(101)
train = sample(1:nrow(boston), 300)
rf.boston = randomForest(medv~., data = boston, subset = train)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry,
ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red",
"blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red",
"blue"))
cbind(test.err, oob.err)
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red",
"blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("Test","OOB"), pch = 23, col = c("red",
"blue"))
set.seed(42)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = training_boston, mtry=mtry,
ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, test_boston)
test.err[mtry] = with(test_boston, mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red",
"blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("Test","OOB"), pch = 23, col = c("red",
"blue"))
set.seed(42)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = training_boston, mtry=mtry,
ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, test_boston)
test.err[mtry] = with(test_boston, mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(oob.err,test.err), pch = 23, col = c("red",
"blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("Test","OOB"), pch = 23, col = c("red",
"blue"))
matplot(1:mtry, cbind(oob.err,test.err), pch = 23, col = c("red",
"blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB","Test"), pch = 23, col = c("red",
"blue"))
cbind(test.err, oob.err)
erros_rf  <- data.frame(
mtry = 1:mtry,
mse_oob = oob.err,
mse_test = test.err
)
erros_rf |>
dplyr::filter(mse_oob == min(mse_oob)) |>
dplyr::pull(mtry)
erros_rf  <- data.frame(
mtry = 1:mtry,
mse_oob = oob.err,
mse_test = test.err
)
erros_rf |>
dplyr::filter(mse_oob == min(mse_oob)) |>
dplyr::pull(mtry)
erros_rf  <- data.frame(
mtry = 1:mtry,
mse_oob = oob.err,
mse_test = test.err
)
erros_rf |>
dplyr::filter(mse_oob == min(mse_oob)) |>
dplyr::pull(mtry)
erros_rf |>
dplyr::filter(mse_test == min(mse_test)) |>
dplyr::pull(mtry)
erros_rf  <- data.frame(
mtry = 1:mtry,
mse_oob = oob.err,
mse_test = test.err
)
mtry_minOOB <-erros_rf |>
dplyr::filter(mse_oob == min(mse_oob)) |>
dplyr::pull(mtry)
mtry_minTest <- erros_rf |>
dplyr::filter(mse_test == min(mse_test)) |>
dplyr::pull(mtry)
print(paste("Número de preditores que resulta no MSE Mínimo no OOB: ", mtry_minOOB))
print(paste("Número de preditores que resulta no MSE Mínimo no Test: ", mtry_minTest))
predict_test <- predict(rf.boston,test_boston,type='class') #prevendo coms os o modelo do treino
predict_test
pairs(boston, main="Boston Data")
?glance
test_boston
aret::MAE(test_boston$medv,predict_test)
caret::MAE(test_boston$medv,predict_test)
predict_test <- predict(rf.boston,test_boston,type='class')
predict_train <- predict(rf.boston,train_boston,type='class')
predict_test <- predict(rf.boston,test_boston,type='class')
predict_train <- predict(rf.boston,training_boston,type='class')
KPIS_regression_train <- data.frame(
mae = caret::MAE(training_boston$medv,predict_train),
rmse = caret::RMSE(training_boston$medv,predict_train)
)
KPIS_regression_test <- data.frame(
mae = caret::MAE(test_boston$medv,predict_test),
rmse = caret::RMSE(test_boston$medv,predict_test)
)
KPIS_regression_train
KPIS_regression_test
predict_test <- predict(rf.boston,test_boston,type='class')
predict_train <- predict(rf.boston,training_boston,type='class')
KPIS_regression_train <- data.frame(
mae = caret::MAE(training_boston$medv,predict_train),
rmse = caret::RMSE(training_boston$medv,predict_train)
)
KPIS_regression_train
KPIS_regression_test <- data.frame(
mae = caret::MAE(test_boston$medv,predict_test),
rmse = caret::RMSE(test_boston$medv,predict_test)
)
KPIS_regression_test
help("MAE")
?mae
?RMSE
?rmse
cbind(predict_test,test_boston$medv)
cbind(predict_test,test_boston$medv) |> scatter.smoot)
cbind(predict_test,test_boston$medv) |> scatter.smoot())
cbind(predict_test,test_boston$medv) |> scatter.smoot()
cbind(predict_test,test_boston$medv) |> scatter.smooth()
comparativo_teste <- cbind(predict_test,test_boston$medv)
names(comparativo_teste) <- c("y_pred","y_true")
comparativo_teste |>
ggplot(aes(x=y_pred,y=y_true)) +
geom_point() +
geom_smooth() +
theme_classic()
comparativo_teste <- cbind(predict_test,test_boston$medv)
names(comparativo_teste) <- c("y_pred","y_true")
comparativo_teste
predict_test
cbind(predict_test,test_boston$medv)
comparativo_teste <- data.frame(y_pred = predict_test,
y_true=test_boston$medv)
comparativo_teste |>
ggplot(aes(x=y_pred,y=y_true)) +
geom_point() +
geom_smooth() +
theme_classic()
install.packages("gbm")
require(gbm)
library(gbm)
boost.boston = gbm(medv~., data = training_boston, distribution =
"gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
help("Boston")
require(gbm)
library(gbm)
boost.boston = gbm(medv~., data = training_boston, distribution =
"gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
plot(boost.boston,i="nox")
plot(boost.boston,i="age")
plot(boost.boston,i="dis")
boost.boston
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = training_boston, n.trees =
n.trees)
dim(predmat)
boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean))
plot(n.trees, boost.err, pch = 23, ylab = "Erro Médio Quadrado", xlab
= "Número de Árvores", main = "Teste do Erro de Boosting")
boost.err = with(test_boston, apply( (predmat - medv)^2, 2, mean))
plot(n.trees, boost.err, pch = 23, ylab = "Erro Médio Quadrado", xlab
= "Número de Árvores", main = "Teste do Erro de Boosting")
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = test_boston, n.trees =
n.trees)
dim(predmat)
boost.err = with(test_boston, apply( (predmat - medv)^2, 2, mean))
plot(n.trees, boost.err, pch = 23, ylab = "Erro Médio Quadrado", xlab
= "Número de Árvores", main = "Teste do Erro de Boosting")
for(col in names(select(boston,-c(medv)))){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
#geom_smooth(method=lm , color="red", se=FALSE) +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
boston
names(select(boston,-c(medv)
names(select(boston,-c(medv))
names(select(boston,-c(medv)))
boston |>
select(-c(medv))
for(col in names(select(boston,select=-c(medv)))){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
#geom_smooth(method=lm , color="red", se=FALSE) +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
for(col in names(subset(boston,select=-c(medv)))){
p <- ggplot(boston, aes(y=medv, x=!!sym(col))) +
geom_point() +
#geom_smooth(method=lm , color="red", se=FALSE) +
labs(title = paste("medv vs ",col))+
theme(legend.position="none")
p1 <- ggMarginal(p, type="histogram", size=10,fill = "darkgreen")
print(p1)
}
